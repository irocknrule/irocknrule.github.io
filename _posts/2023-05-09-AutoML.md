# Using AutoGluon on Tabular datasets

## Why use AutoGluon?

AutoGluon is an open source package within the class of AutoML techniques to make machine learning easier and more accessible. The description of AutoGluon is stated as (from [1]):

AutoGluon enables easy-to-use and easy-to-extend AutoML with a focus on automated stack ensembling, deep learning, and real-world applications spanning image, text, and tabular data. Intended for both ML beginners and experts, 

AutoGluon enables you to:

- Quickly prototype deep learning and classical ML solutions for your raw data with a few lines of code.
- Automatically utilize state-of-the-art techniques (where appropriate) without expert knowledge.
- Leverage automatic hyperparameter tuning, model selection/ensembling, architecture search, and data processing.
- Easily improve/tune your bespoke models and data pipelines, or customize AutoGluon for your use-case.

I was interested to try to use this on a real world example to explore the possibilities and learn more about model building using this tool. Online research indicated that it is particularly useful for modeling tabular datasets out of the box, so I selected the American Express — Default Prediction Kaggle competition. The advantages of using a Kaggle dataset are:

- Datasets are public, so there are no worries regarding disclosing work-related confidential information.
- The Kaggle community is large and numerous people publish their modeling techniques. While some folks are serious about winning, most people use it to learn various modeling approaches and use their results to compare against the state-of-the-art.
- The public and private leaderboards provide a ready ability to evaluate the designed models.

Using AutoGluon to come up with a starting model for this competition provides an opportunity to come up with a baseline model with minimal starting effort. Once we have a baseline model, then we could work more on fine-tuning, hyper parameter optimization , feature engineering and other architectural improvements to improve the overall accuracy.

## Data set and goals

The official input datasets for the Kaggle Amex competition are provided are provided here. Due to the large size of the dataset, folks in the community converted the input CSVs into a smaller parquet format which was widely used. For this analysis, I have re-used this dataset which was shared publicly at this location [2]. The competition also provided a specific evaluation metric as defined here, however for simplicity and again ease of use with the goal of evaluation AutoGluon, I used the standard ‘recall’ metric to compute model scores.

## Autogluon modeling

I used a Sagemaker ml.g4dn.4xlarge instance which has enough memory and a GPU to enable faster model training. By specifying the ag_args_fit={‘num_gpus’: 1} parameter to fit, we can specify usage of the GPU instead of the CPU whenever possible.

After installing AutoGluon locally, you simply import the Tabular libraries directly:

```from autogluon.tabular import TabularDataset, TabularPredictor```

After reading the training data (using the parquet files) we convert some of the pre-specified columns to categorical variables explicitly and then invoke AutoGluon fitting.

A quick note here, AutoGluon enables fast prototyping and modeling with smaller training datasets, time limits and little hyper-parameter tuning to ensure we can get some working code up and running. After some experimentation with smaller subsample sizes and letting AutoGluon fit all 13+ models, I observed that some models work better than others and ensembling them according to AutoGluon’s layered stacking strategies gives us good results. I landed upon using the following modeling techniques, which can then be passed in as a dict to the *fit* function.

```
hyperparams = { # hyperparameters of each model type
              ‘GBM’: {}, # Gradient-Boosting Trees
              ‘XGB’: {}, # XGBoost
              ‘FASTAI’: {}, # Neural nets from FastIA
              ‘CAT’: {}, # CatBoost
              ‘NN_TORCH’: {} # Neural nets from PyTorch
 }
```

Note that I have not specified individual hyper-parameters for every model, which can be included as well. This is because in the call to the fit function, I specify that we want the ‘best quality’, so AutoGluon automatically selects the best set of hyper parameters to search. Since my goal is to measure how AutoGluon performs right out the gate, without any specific feature engineering or optimizations, I kept the default options. Here is the call to the *fit* method:

```
predictor = TabularPredictor(label='target',
                             eval_metric='recall',)
                             .fit(train_df,ag_args_fit={'num_gpus': 1}
                             ,hyperparameters=hyperparams
                             ,time_limit=4*60*60,presets=["best_quality"])
```

AutoGluon then starts the training process, which we have specified to have an time limit of 4 hours, use the GPU when possible and use recall as our evaluation metric. 

Some sample output from the process indicates how AutoGluon starts working here:

```
No path specified. Models will be saved in: "AutogluonModels/ag-20220924_172812/"
Presets specified: ['best_quality']
Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=20
Beginning AutoGluon training ... Time limit = 14400s
AutoGluon will save models to "AutogluonModels/ag-20220924_172812/"
AutoGluon Version:  0.5.2
Python Version:     3.8.12
Operating System:   Linux
Train Data Rows:    458913
Train Data Columns: 189
Label Column: target
Preprocessing data ...
AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).
	2 unique label values:  [0, 1]
	If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])
Selected class <--> label mapping:  class 1 = 1, class 0 = 0
Using Feature Generators to preprocess the data ...
Fitting AutoMLPipelineFeatureGenerator...
	Available Memory:                    34811.0 MB
	Train Data (Original)  Memory Usage: 827.85 MB (2.4% of available memory)
	Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.
	Stage 1 Generators:
		Fitting AsTypeFeatureGenerator...
	Stage 2 Generators:
		Fitting FillNaFeatureGenerator...
	Stage 3 Generators:
		Fitting IdentityFeatureGenerator...
		Fitting CategoryFeatureGenerator...
			Fitting CategoryMemoryMinimizeFeatureGenerator...
	Stage 4 Generators:
		Fitting DropUniqueFeatureGenerator...
	Unused Original Features (Count: 1): ['customer_ID']
		These features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.
		Features can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.
		These features do not need to be present at inference time.
		('object', []) : 1 | ['customer_ID']
	Types of features in original data (raw dtype, special dtypes):
		('float', [])  : 177 | ['P_2', 'D_39', 'B_1', 'B_2', 'R_1', ...]
		('object', []) :  11 | ['D_63', 'D_64', 'D_66', 'D_68', 'B_30', ...]
	Types of features in processed data (raw dtype, special dtypes):
		('category', []) :  11 | ['D_63', 'D_64', 'D_66', 'D_68', 'B_30', ...]
		('float', [])    : 177 | ['P_2', 'D_39', 'B_1', 'B_2', 'R_1', ...]
	11.9s = Fit runtime
	188 features in original data used to generate 188 features in processed data.
	Train Data (Processed) Memory Usage: 484.16 MB (1.4% of available memory)
Data preprocessing and feature engineering runtime = 13.68s ...
AutoGluon will gauge predictive performance using evaluation metric: 'recall'
	To change this, specify the eval_metric parameter of Predictor()
Fitting 5 L1 models ...
Fitting model: LightGBM_BAG_L1 ... Training model for up to 14386.32s of the 14386.31s of remaining time.
```

We can observe that it automatically deduces that this is a binary classification problem, the customer_ID field is not very useful, fills NA values and the goes about the training process. Once it is done (in this case it takes close to 3.5 hours) the output models are saved on disk in the ```AutogluonModels/ag-20220924_172812/``` location.

## Predictor Leaderboards

We now want to check the best models for our data, which can be observed by looking at the predictor leaderboards.

```predictor.leaderboard(extra_info=True, silent=True)```

Output shows that the WeightedEnsemble_L2 model has the highest score. This ensemble is created using the NeuralNetFastAI and NeuralNetTorch neural networks.

![](/images/autogluon_output.webp "Auto Gluon's selected best ensemble model")

## Testing and Kaggle Submission

We now load the testing dataset from Kaggle to generate the final predictions from our WeightedEnsemble_L2 model. Since the test dataset in itself is large as well, doing inference on hundreds of thousands of rows is very memory intensive. So we chunk the test dataframe into an arbitrary number of chunks (here we use 250) and then predict the probabilities that the output is a default, i.e. the target value is 1.

```
n = 250
list_df = np.array_split(test_data_df, n)
y_pred = []
y_true = []
for i in range(0,n):
    df_chunk = list_df[i]
    y_pred.append(best_predictor.predict_proba(df_chunk).iloc[:, 1:3])
test_preds = pd.concat(y_pred, axis=0, ignore_index=True)
```

The final predictions are then saved to a CSV and submitted to Kaggle.

```
test_preds_df.to_csv("final_submission.csv",index=False)
!kaggle competitions submit -c amex-default-prediction -f final_submission.csv -m "Final Submission"
```

## Public and Private Leaderboard Scores

Moment of truth, how did the AutoGluon models perform. Keep in mind that we did no feature engineering at all in the input dataset, selected some models which looked to be performing well in smaller subsets of data and trained for less than 4 hours. For this particular submission, the scores were:

![](/images/autogluon_scores.webp "Private and Public scores on Kaggle leaderboards") 

So comparing the 0.77538 for private and 0.75987 for public scores on the overall leaderboard is a very decent score in relation to the top private score of 0.80977. Now of course (at the the time of writing this) there are about 3700 submissions between rank 1 vs the score we ended up at (0.77538), but in my opinion this an extremely good score as a starting point to build up on.

## Conclusions and Thoughts

The primary goal here was not to come up with a model in the top 10 for this Kaggle competition, which in general is super competitive with grandmasters spending days and weeks fine tuning their models to get the most optimal score. Here we wanted to evaluate how we can effectively use AutoGluon to come up with a baseline model and gain some insight. If this was a specific real world use case, then I would plan on using this baseline number to actually start the modeling process in earnest. For eg, we saw that FastAI’s neural network architecture was performing very well as per AutoGluon’s findings. At this point, I would take this data to the FastAI library and then work further on optimizing, feature engineering and training the model longer to come up with a better model. On the other hand, if the accuracy of this model from AutoGluon was good enough for our real world use case (and we were short on time), then I would go ahead and work on pushing this model to production.

In some sense, AutoGluon makes things easier for the ML practitioner to get started and I am all for it.

## Link to notebooks

The training and testing notebooks are located here.

<https://github.com/irocknrule/automl-notebooks>

## References

[1] AutoGluon: <https://auto.gluon.ai/stable/index.html>

[2] Dataset converted to parquet format by user radarr: <https://www.kaggle.com/datasets/raddar/amex-data-integer-dtypes-parquet-format>
